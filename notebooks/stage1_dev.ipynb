# stage1_dev.ipynb
import torch
from models.hrm_v4 import HRM_V4
from models.tokenizer import CharTokenizer
from utils.collate_v4 import pad_action_sequences
from utils.visualization import plot_maze

# ----------------------
# Dummy setup
# ----------------------
maze_size = 50
batch_size = 2
seq_len = 10

# Dummy maze: 0=open, 1=wall
dummy_mazes = torch.zeros((batch_size,1,maze_size,maze_size))
dummy_paths = [list(range(seq_len)) for _ in range(batch_size)]

# Initialize model
tokenizer = CharTokenizer()
model = HRM_V4(embed_dim=256, high_hidden=512, low_hidden=512,
               n_actions=4, vocab_size=tokenizer.vocab_size)

# Forward pass (maze)
padded, max_len = pad_action_sequences(dummy_paths)
logits = model(dummy_mazes, seq_len=max_len, task='maze')
print("Maze logits shape:", logits.shape)

# Forward pass (text)
dummy_text_mazes = torch.zeros((batch_size,1,maze_size,maze_size))
logits_text = model(dummy_text_mazes, seq_len=seq_len, task='text')
print("Text logits shape:", logits_text.shape)

# Optional: visualize a maze
import numpy as np
maze_np = np.random.randint(0,2,(maze_size,maze_size))
plot_maze(maze_np, path=[(i,i) for i in range(maze_size)])
